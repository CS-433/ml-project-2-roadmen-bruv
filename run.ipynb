{"cells":[{"cell_type":"markdown","metadata":{"id":"dv9LvNvH0UwW"},"source":["# **Machine Learning: Project 2 - Road segmentation**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"lb1NwAeSlGYH"},"source":["# I) Setup"]},{"cell_type":"markdown","metadata":{"id":"eTqRcJFglEQw"},"source":["## Drive/repository access"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20124,"status":"ok","timestamp":1702824476136,"user":{"displayName":"Tomas Valdivieso","userId":"06975159853397808043"},"user_tz":-60},"id":"AMAsrtnMBVG1","outputId":"2cbbbf06-e726-438b-d5fd-a7a38d1dc23f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1702824476138,"user":{"displayName":"Tomas Valdivieso","userId":"06975159853397808043"},"user_tz":-60},"id":"NHorZ12T7Oyj","outputId":"6b3b04b7-b333-4962-e8cb-372a3fe86da3"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ML_google_colab/Project 2/ml-project-2-roadmen-bruv\n"]}],"source":["## Folder path to Google Drive project directory\n","\n","victor = False\n","tomas = True\n","edwin = False\n","\n","\n","if tomas:\n","  path_to_your_folder =  \"/content/drive/MyDrive/ML_google_colab/Project 2/ml-project-2-roadmen-bruv\"\n","elif victor:\n","  path_to_your_folder = \"/content/drive/MyDrive/EPFL/MachineLearningMA3/ml-project-2-roadmen-bruv\"\n","elif edwin:\n","  path_to_your_folder = \"/content/drive/MyDrive/ml-project-2-roadmen-bruv\"\n","\n","\n","%cd $path_to_your_folder"]},{"cell_type":"markdown","metadata":{"id":"D4romuDEk8Qe"},"source":["## Libraries installation and imports"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":26699,"status":"ok","timestamp":1702824502826,"user":{"displayName":"Tomas Valdivieso","userId":"06975159853397808043"},"user_tz":-60},"id":"32osBUOECMEs"},"outputs":[],"source":["from IPython.display import clear_output\n","!pip install git+https://github.com/qubvel/segmentation_models.pytorch --quiet\n","!pip install -U albumentations --quiet\n","clear_output()"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":736,"status":"ok","timestamp":1702825882936,"user":{"displayName":"Tomas Valdivieso","userId":"06975159853397808043"},"user_tz":-60},"id":"rslVf64mG26i","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b0e33a3a-9a46-4f2e-8454-d8e72bf5b608"},"outputs":[{"output_type":"stream","name":"stdout","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["%load_ext autoreload\n","%autoreload 2\n","%matplotlib inline\n","\n","import segmentation_models_pytorch as smp\n","from segmentation_models_pytorch import utils as smp_utils\n","import albumentations as albu\n","\n","import sys\n","sys.path.append(\"./utils\")\n","sys.path.append(\"./helpers\")\n","\n","import matplotlib.image as mpimg\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import cv2\n","import os\n","import torch\n","\n","from torch.utils.data import DataLoader\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","import pandas as pd\n","\n","## Utils\n","from save_training_results import save_results\n","from data_augmentation import load_img_training, split_keys, store_images, resize_augment_store_dataset, confirm_and_augment\n","from dataset import Dataset, get_preprocessing\n","\n","## Helpers\n","from mask_to_submission import masks_to_submission\n","\n","\n","## To empty cache\n","import gc ###\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"t7hdYfDMlSND"},"source":["# II) Model definition"]},{"cell_type":"markdown","metadata":{"id":"XQor9tdyk-wl"},"source":["## Command board"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":563,"status":"ok","timestamp":1702826532991,"user":{"displayName":"Tomas Valdivieso","userId":"06975159853397808043"},"user_tz":-60},"id":"fuY2yG0t-OYI"},"outputs":[],"source":["PARAMS = {\n","  'MODELS' : [\"Unet\"], # Available : \"Unet\",\"DeepLabV3\",\"FPN\", \"UnetPlusPlus\"\n","  'ENCODER' : 'resnet34',\n","  'ENCODER_WEIGHTS' : 'imagenet',\n","  'NB_EPOCHS' : 1,\n","  'ACTIVATION' : 'sigmoid', # could be None for logits or 'softmax2d' for multiclass segmentation,\n","  'DATA_AUGMENTATION' : True, #choose whether to train with augmented dataset,\n","  'LOSS_TYPE': \"tversky\", # Possible: [\"dice\", \"tversky\", \"custom\"]\n","  'METRIC_TYPE': \"fscore\", # Possible: [\"custom\", \"fscore\"]\n","  'THRESHOLD' : 0.5,  # Threshold for determining foreground vs background\n","  'CLASSES' : ['road'],\n","  'AVERAGE' : False  # Each image is predicted 4 times (once in each orientation) and the average of those predictions is used as the AICrowd submission\n","}\n","\n","TVERSKY = {\n","    'alpha' : 0.3,\n","    'beta' : 0.3,\n","    'gamma' : 0.75,\n","}"]},{"cell_type":"markdown","source":["## Augment the data (only needs to be done once)"],"metadata":{"id":"y_rqdW2ZZdQc"}},{"cell_type":"code","source":["# Create folders for data augmentation and store augmented data\n","confirm_and_augment()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":364,"referenced_widgets":["7289311d1c714adca9c511f0c30726ee","f6d4419449814d3aab64a76585959b27","0d48e9c4709c4b6dad4daab1b4d1bd63","f376942710a543fc85de3d79ad12869b","b93515d9e1c0405997d94a37cf3712b4","9bbe0da652d74846b63db68b5553eaa9","1550fa8a0b0846b89c012821846f4848","03485ca245434455a9f080ae389dc57e"]},"id":"e8kgoz0gWLQ7","executionInfo":{"status":"ok","timestamp":1702826460692,"user_tz":-60,"elapsed":314,"user":{"displayName":"Tomas Valdivieso","userId":"06975159853397808043"}},"outputId":"61d1e333-e35a-4e4d-abaf-ae6fc5c25d9c"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Proceeding with data augmentation...\n","Directory data/data_train_augmented already exists\n","Directory data/data_train_augmented/images/ already exists\n","Directory data/data_train_augmented/masks/ already exists\n","Directory data/data_train_augmented/raw/ already exists\n","Directory data/data_train_augmented/raw/images/ already exists\n","Directory data/data_train_augmented/raw/masks/ already exists\n","Directory data/data_validation already exists\n","Directory data/data_validation/images/ already exists\n","Directory data/data_validation/masks/ already exists\n","Directory data/data_validation/raw/ already exists\n","Directory data/data_validation/raw/images/ already exists\n","Directory data/data_validation/raw/masks/ already exists\n","Images stored in ./data/data_train_augmented/raw/images/\n","Images stored in ./data/data_train_augmented/raw/masks/\n","Images stored in ./data/data_validation/raw/images/\n","Images stored in ./data/data_validation/raw/masks/\n","Images stored in ./data/data_validation/images/\n","Images stored in ./data/data_validation/masks/\n","Images stored in ./data/data_train_augmented/images/\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VG8fw7Xz_g2M"},"outputs":[],"source":["\n","#Creates the necessary folders for saving results\n","model_weights_folder = './submissions/models/'\n","os.makedirs(model_weights_folder, exist_ok=True)\n","\n","\n","submission_folder = './submissions/'\n","os.makedirs(submission_folder, exist_ok=True)"]},{"cell_type":"markdown","source":["## Creating training and validation set"],"metadata":{"id":"cy44lJ5PZQ68"}},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"aborted","timestamp":1702825883299,"user":{"displayName":"Tomas Valdivieso","userId":"06975159853397808043"},"user_tz":-60},"id":"1CHZmMQaaykZ"},"outputs":[],"source":["## Importing preprocessing fonction\n","preprocessing_fn = smp.encoders.get_preprocessing_fn(PARAMS[\"ENCODER\"], PARAMS[\"ENCODER_WEIGHTS\"])\n","\n","#Defining folder paths\n","PATH_TR_IMG_AUG_RAW = \"./data/data_train_augmented/raw/images/\"\n","PATH_TR_MASK_AUG_RAW = \"./data/data_train_augmented/raw/masks/\"\n","PATH_VAL_IMG_RAW = \"./data/data_validation/raw/images/\"\n","PATH_VAL_MASK_RAW = \"./data/data_validation/raw/masks/\"\n","PATH_TR_IMG_AUG = \"./data/data_train_augmented/images/\"\n","PATH_TR_MASK_AUG = \"./data/data_train_augmented/masks/\"\n","PATH_VAL_IMG = \"./data/data_validation/images/\"\n","PATH_VAL_MASK = \"./data/data_validation/masks/\"\n","\n","\n","#change paths for the training and validation datasets depending on wether we want data augmentation or not\n","if PARAMS[\"DATA_AUGMENTATION\"]:\n","  training_path_img = PATH_TR_IMG_AUG\n","  training_path_mask = PATH_TR_MASK_AUG\n","  validation_path_img = PATH_VAL_IMG\n","  validation_path_mask = PATH_VAL_MASK\n","else:\n","  training_path_img = PATH_TR_IMG_AUG_RAW\n","  training_path_mask = PATH_TR_MASK_AUG_RAW\n","  validation_path_img = PATH_VAL_IMG_RAW\n","  validation_path_mask = PATH_VAL_MASK_RAW\n","\n","#create training and validation datasets\n","train_dataset = Dataset(\n","    training_path_img,\n","    training_path_mask,\n","    preprocessing=get_preprocessing(preprocessing_fn),\n","    classes=[\"road\"])\n","\n","\n","valid_dataset = Dataset(\n","    validation_path_img,\n","    validation_path_mask,\n","    preprocessing=get_preprocessing(preprocessing_fn),\n","    classes=[\"road\"],\n",")\n","\n","#create the loaders for both datasets\n","train_loader = DataLoader(train_dataset, batch_size=30, shuffle=False)\n","valid_loader = DataLoader(valid_dataset, batch_size=20, shuffle=False)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xNLXJTNEn91d"},"source":["### Patch based Loss and evaluation metrics\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1702825883299,"user":{"displayName":"Tomas Valdivieso","userId":"06975159853397808043"},"user_tz":-60},"id":"764Vgxn2ayka"},"outputs":[],"source":["from f1scorepatch import F1ScorePatch\n","\n","if PARAMS[\"LOSS_TYPE\"] == \"dice\":\n","  loss_fn = smp.losses.dice.DiceLoss(mode ='binary')\n","  loss_fn.__name__ = 'Dice_loss'\n","  loss_name = 'Dice_loss'\n","elif PARAMS[\"LOSS_TYPE\"] == \"tversky\":\n","  loss_fn = smp.losses.tversky.TverskyLoss(mode ='binary', alpha=TVERSKY[\"alpha\"] , beta=TVERSKY[\"beta\"], gamma=TVERSKY[\"gamma\"])\n","  loss_fn.__name__ = 'Tversky_Loss'\n","  loss_name = 'Tversky_Loss'\n","\n","\n","if PARAMS[\"METRIC_TYPE\"] == \"fscore\":\n","  metrics_training = [smp_utils.metrics.Fscore()]\n","  metrics_validation = [smp_utils.metrics.Fscore(), F1ScorePatch(activation='sigmoid')]\n","  metric_name_val = \"f1score_patch\" # can also pick \"fscore\""]},{"cell_type":"markdown","metadata":{"id":"l-P1uVn8mAIN"},"source":["# III) Training"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1702825883299,"user":{"displayName":"Tomas Valdivieso","userId":"06975159853397808043"},"user_tz":-60},"id":"Cd2imBFfaykX"},"outputs":[],"source":["#Instantiating all the models\n","models = [[smp.create_model(model_name, encoder_name=PARAMS[\"ENCODER\"], encoder_weights = PARAMS[\"ENCODER_WEIGHTS\"], in_channels=3, classes=1),model_name] for model_name in PARAMS[\"MODELS\"]]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1702825883299,"user":{"displayName":"Tomas Valdivieso","userId":"06975159853397808043"},"user_tz":-60},"id":"h2DZo52faykc"},"outputs":[],"source":["# train models for NB_EPOCHS\n","\n","for model,model_name in models:\n","  print(model_name)\n","  optimizer = torch.optim.Adam([\n","    dict(params=model.parameters(), lr=4.559e-4),\n","])\n","\n","  train_epoch = smp.utils.train.TrainEpoch(\n","      model,\n","      loss=loss_fn,\n","      metrics=metrics_training,\n","      optimizer=optimizer,\n","      device=\"cuda\",\n","      verbose=True,\n","  )\n","\n","  valid_epoch = smp.utils.train.ValidEpoch(\n","      model,\n","      loss=loss_fn,\n","      metrics= metrics_validation,\n","      device=\"cuda\",\n","      verbose=True,\n","  )\n","\n","\n","  max_score = 0\n","  train_loss_array = []\n","  validation_loss_array = []\n","  validation_fscore_array = []\n","\n","  scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n","\n","  for i in range(0, PARAMS[\"NB_EPOCHS\"]):\n","\n","      print('\\nEpoch: {}'.format(i))\n","      train_logs = train_epoch.run(train_loader)\n","      valid_logs = valid_epoch.run(valid_loader)\n","\n","      train_loss_array.append(train_logs[loss_name])\n","      validation_loss_array.append(valid_logs[loss_name])\n","      validation_fscore_array.append(valid_logs[metric_name_val])\n","      print(valid_logs)\n","\n","      scheduler.step(valid_logs[metric_name_val])\n","\n","      if max_score < valid_logs[metric_name_val]:\n","          max_score = valid_logs[metric_name_val]\n","          torch.save(model, model_weights_folder + 'best_model_{}.pth'.format(model_name))\n","          print('Model saved!')\n","\n","\n","  epochs = range(0,len(train_loss_array))\n","  save_results(PARAMS,train_loss_array,validation_loss_array,validation_fscore_array)\n","\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(epochs, train_loss_array,\"o\", label='Training Loss')\n","plt.plot(epochs, validation_loss_array,  label='Validation Loss')\n","plt.plot(epochs, validation_fscore_array, \"o\" ,  label='Validation fscore')\n","plt.title('Training and Validation Loss for {}'.format(model_name))\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gb7gjUmf17ON"},"source":["# IV) Submission\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1702825883300,"user":{"displayName":"Tomas Valdivieso","userId":"06975159853397808043"},"user_tz":-60},"id":"HBx4LAICmMHa"},"outputs":[],"source":["MODEL_NAME = \"DeepLabV3\"\n","# Modify according to model saved that you want to import\n","MODEL_PATH = model_weights_folder + f'best_model_{MODEL_NAME}.pth'\n","DEVICE ='cuda'\n","test_model = torch.load(MODEL_PATH)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1702825883300,"user":{"displayName":"Tomas Valdivieso","userId":"06975159853397808043"},"user_tz":-60},"id":"Q56JyGLW17OS"},"outputs":[],"source":["test_dataset = Dataset(\n","    images_dir=\"./data/test_set_images/\",\n","    preprocessing= get_preprocessing(preprocessing_fn),\n","    classes=['road'])\n","\n","test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)"]},{"cell_type":"markdown","metadata":{"id":"3REiwYfLnMEF"},"source":["## Submission creation"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1702825883300,"user":{"displayName":"Tomas Valdivieso","userId":"06975159853397808043"},"user_tz":-60},"id":"UChWeShx0lXC"},"outputs":[],"source":["\n","### with average of multiple predictions\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","test_model = test_model.to(DEVICE)\n","\n","# Parameters for saving images\n","output_folder = submission_folder + 'predicted_masks/'\n","os.makedirs(output_folder, exist_ok=True)\n","\n","# Iterate over the DataLoader\n","for i, (path, image) in enumerate(test_loader):\n","    path = path[0]\n","    with torch.no_grad():\n","        test_model.eval()\n","        # Move input to the device\n","        input_tensor = image.to(DEVICE)\n","        n, c, h, w = input_tensor.size()\n","        sum = torch.zeros((n, 1, h, w), device='cuda')\n","        if PARAMS[\"AVERAGE\"]:\n","          for i in range(4):\n","            img_rot = torch.rot90(input_tensor, i%4, [2, 3])\n","            prediction = test_model(img_rot)\n","            prediction = torch.rot90(prediction, 4-(i%4), [2, 3]) #rotation back\n","            sum += prediction\n","          pred = sum / 4\n","        else:\n","          pred = test_model(input_tensor)\n","\n","\n","        pred_np = pred.detach().cpu().numpy()[0, 0]\n","        pred_np = 1 / (1 + np.exp(-pred_np))\n","        pred_voting = (pred_np > 0.5).astype(float)*255\n","\n","        # Save the image\n","        image_num = path.split('/')[-1].split('_')[-1].split('.')[0]\n","        image_num = int(image_num)\n","        filename = \"pr_img_\" + '%.3d' % image_num + '.png'\n","\n","\n","        cv2.imwrite(os.path.join(output_folder, filename),pred_voting)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1702825883300,"user":{"displayName":"Tomas Valdivieso","userId":"06975159853397808043"},"user_tz":-60},"id":"OgXcwZ4K17OX"},"outputs":[],"source":["submission_filename = submission_folder + '{}.csv'.format(MODEL_NAME)\n","image_filenames = []\n","for i in range(1, 51):\n","    image_filename = output_folder + 'pr_img_' + '%.3d' % i + '.png'\n","    image_filenames.append(image_filename)\n","masks_to_submission(submission_filename, *image_filenames)\n","\n","## basically the order of predicted doesnt match order of test files"]},{"cell_type":"markdown","metadata":{"id":"d_cqzcgW-6uS"},"source":["## Visualisation predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1702825883300,"user":{"displayName":"Tomas Valdivieso","userId":"06975159853397808043"},"user_tz":-60},"id":"e52giDyD-47e"},"outputs":[],"source":["\n","def list_files_in_directory(directory):\n","    file_list = []\n","    for root, dirs, files in os.walk(directory):\n","        for file in files:\n","            file_list.append(os.path.join(root, file))\n","    return file_list\n","\n","\n","pred_files = list_files_in_directory(output_folder)\n","pred_files = sorted(pred_files, key=lambda x: int(os.path.basename(x)[7:-4]))\n","\n","#could sort pred files and images files\n","img_path = \"./data/test_set_images/\"\n","img_files = list_files_in_directory(img_path)\n","img_files = sorted(img_files, key=lambda x: int(os.path.basename(x)[5:-4]))\n","img_mask_list = []\n","for i, pred_file in enumerate(pred_files):\n","    img = cv2.imread(img_files[i], cv2.IMREAD_COLOR)\n","    pred_mask = cv2.imread(pred_file, cv2.IMREAD_GRAYSCALE)\n","\n","    # Create a semi-transparent green mask\n","    alpha = 0.7\n","    red_mask = np.zeros_like(img)\n","    for (i,j), value in np.ndenumerate(pred_mask):\n","        if value == 255:\n","            red_mask[i,j] = [0, 255, 0]\n","\n","    # Combine image and the green mask\n","    result = cv2.addWeighted(img, 1, red_mask, alpha, 0)\n","    img_mask_list.append(result)\n","# Set the number of images per row\n","images_per_row = 5\n","\n","# Calculate the number of rows needed\n","num_rows = (len(img_mask_list) + images_per_row - 1) // images_per_row\n","\n","# Create a single row of subplots\n","fig, axs = plt.subplots(1, images_per_row, figsize=(10, 5))\n","\n","# Loop through files and plot images\n","for i, img in enumerate(img_mask_list):\n","    axs[i % images_per_row].imshow(img)\n","    axs[i % images_per_row].axis('off')\n","\n","    # If last image in the row, create a new row of subplots\n","    if (i + 1) % images_per_row == 0:\n","        plt.show()\n","        if i + 1 < len(img_mask_list):\n","            fig, axs = plt.subplots(1, images_per_row, figsize=(10, 5))\n","\n","plt.show()\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"7289311d1c714adca9c511f0c30726ee":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f6d4419449814d3aab64a76585959b27","IPY_MODEL_0d48e9c4709c4b6dad4daab1b4d1bd63"],"layout":"IPY_MODEL_f376942710a543fc85de3d79ad12869b"}},"f6d4419449814d3aab64a76585959b27":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Yes","disabled":false,"icon":"","layout":"IPY_MODEL_b93515d9e1c0405997d94a37cf3712b4","style":"IPY_MODEL_9bbe0da652d74846b63db68b5553eaa9","tooltip":""}},"0d48e9c4709c4b6dad4daab1b4d1bd63":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"No","disabled":false,"icon":"","layout":"IPY_MODEL_1550fa8a0b0846b89c012821846f4848","style":"IPY_MODEL_03485ca245434455a9f080ae389dc57e","tooltip":""}},"f376942710a543fc85de3d79ad12869b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b93515d9e1c0405997d94a37cf3712b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bbe0da652d74846b63db68b5553eaa9":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"1550fa8a0b0846b89c012821846f4848":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03485ca245434455a9f080ae389dc57e":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}}}}},"nbformat":4,"nbformat_minor":0}