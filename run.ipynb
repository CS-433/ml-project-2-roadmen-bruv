{"cells":[{"cell_type":"markdown","source":["# **Machine Learning: Project 2 - Road segmentation**\n","\n","---"],"metadata":{"id":"dv9LvNvH0UwW"}},{"cell_type":"markdown","source":["# I) Setup"],"metadata":{"id":"lb1NwAeSlGYH"}},{"cell_type":"markdown","source":["## Drive/repository access"],"metadata":{"id":"eTqRcJFglEQw"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3157,"status":"ok","timestamp":1701985951326,"user":{"displayName":"Edwin Bertschy","userId":"01520589558980144494"},"user_tz":-60},"id":"AMAsrtnMBVG1","outputId":"e3e7eb73-322f-4dbb-9602-330f509aa3b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NHorZ12T7Oyj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701985951326,"user_tz":-60,"elapsed":6,"user":{"displayName":"Edwin Bertschy","userId":"01520589558980144494"}},"outputId":"cbd9568f-8eb1-420b-f645-978fd9a70775"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ml-project-2-roadmen-bruv\n"]}],"source":["victor = False\n","tomas = False\n","edwin = False\n","\n","if tomas:\n","  path_to_your_folder =  \"/content/drive/MyDrive/ML_google_colab/Project 2/ml-project-2-roadmen-bruv\"\n","elif victor:\n","  path_to_your_folder = \"/content/drive/MyDrive/EPFL/MachineLearningMA3/ml-project-2-roadmen-bruv\"\n","elif edwin:\n","  path_to_your_folder = \"/content/drive/MyDrive/ml-project-2-roadmen-bruv\"\n","\n","%cd $path_to_your_folder"]},{"cell_type":"markdown","source":["## Libraries installation and imports"],"metadata":{"id":"D4romuDEk8Qe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"szGMCcPK8F8V"},"outputs":[],"source":["from IPython.display import clear_output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"32osBUOECMEs"},"outputs":[],"source":["!pip install git+https://github.com/qubvel/segmentation_models.pytorch --quiet\n","!pip install -U albumentations --quiet\n","clear_output()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rslVf64mG26i"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n","%matplotlib inline\n","\n","import segmentation_models_pytorch as smp\n","from segmentation_models_pytorch import utils as smp_utils\n","import albumentations as albu\n","\n","import sys\n","sys.path.append(\"./utils\")\n","sys.path.append(\"./helpers\")\n","\n","import matplotlib.image as mpimg\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import cv2\n","import os\n","import torch\n","from PIL import Image\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset as BaseDataset\n","from PIL import Image\n","import pandas as pd\n","from save_training_results import save_results\n","from mask_to_submission import masks_to_submission\n","from data_augmentation import load_img_training, split_keys, store_images, resize_augment_store_dataset\n","\n","## For custom\n","import torch.nn.functional as F\n","from torch.nn.modules.loss import _Loss\n","from typing import Optional, List\n","import torch.nn as nn\n","import segmentation_models_pytorch.utils.base as base\n","import segmentation_models_pytorch.utils.functional as F\n","from segmentation_models_pytorch.base.modules import Activation\n","\n","## For submission\n","from torchvision.transforms.functional import to_pil_image, to_tensor"]},{"cell_type":"code","source":["import gc ###\n","gc.collect()\n","torch.cuda.empty_cache()"],"metadata":{"id":"HTJWG0RgNpcr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# II) Model definition"],"metadata":{"id":"t7hdYfDMlSND"}},{"cell_type":"markdown","source":["## Command board"],"metadata":{"id":"XQor9tdyk-wl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fuY2yG0t-OYI"},"outputs":[],"source":["PARAMS = {\n","  'MODELS' : [\"Unet\"], # Available : \"Unet\",\"DeepLabV3\",\"FPN\", \"UnetPlusPlus\"\n","  'ENCODER' : 'resnet34',\n","  'ENCODER_WEIGHTS' : 'imagenet',\n","  'NB_EPOCHS' : 10,\n","  'ACTIVATION' : 'sigmoid', # could be None for logits or 'softmax2d' for multiclass segmentation,\n","  'DATA_AUGMENTATION' : False, #choose whether the data is augmented to 900 images or use original dataset of 100 images,\n","  'CLASSES' : ['road'],\n","}"]},{"cell_type":"code","source":["# Default parameters for various parts\n","foreground_threshold = 0.5  # Threshold for determining foreground vs background\n","\n","loss_type = \"dice\" # Possible: [\"dice\", \"tversky\", \"custom\"]\n","metric_type = \"fscore\" # Possible: [\"custom\", \"fscore\"]\n","\n","#For custom, changes balance between pixel and patch\n","w_pix = 0\n","w_patch = 1"],"metadata":{"id":"Waw9mJIlmuYD"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VG8fw7Xz_g2M"},"outputs":[],"source":["#Creates the necessary folders for saving results\n","model_weights_folder = './submissions/models/'\n","os.makedirs(model_weights_folder, exist_ok=True)\n","\n","SUBMISSION_NAME = \"submission_\" + \"test_run/\"\n","submission_folder = './submissions/' + SUBMISSION_NAME\n","os.makedirs(submission_folder, exist_ok=True)"]},{"cell_type":"markdown","source":["## II.a) Dataset class"],"metadata":{"id":"_dSnxrzXlYgV"}},{"cell_type":"code","source":["class Dataset(BaseDataset):\n","  CLASSES = ['non-road', 'road']\n","\n","  def __init__(self, images_dir, masks_dir=None, classes=None, augmentation=None, preprocessing=None, plot = False):\n","      if masks_dir == None:\n","        self.ids = range(1, 51)\n","        self.images_path = [os.path.join(images_dir, f'test_{idx}/',f'test_{idx}.png') for idx in self.ids]\n","      else:\n","        self.ids = os.listdir(images_dir)\n","        self.images_path = [os.path.join(images_dir, image_id) for image_id in self.ids]\n","      self.masks_path = [os.path.join(masks_dir, image_id) for image_id in self.ids] if masks_dir is not None else None\n","\n","      # convert str names to class values on masks\n","      if classes is not None:\n","          self.class_values = [self.CLASSES.index(cls.lower())*255 for cls in classes]\n","\n","      self.augmentation = augmentation ###plus besoin ?\n","      self.preprocessing = preprocessing\n","      # self.preprocessing = None\n","      self.plot = plot\n","\n","  def __getitem__(self, i):\n","\n","      # read data\n","      image = cv2.imread(self.images_path[i])\n","      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","      height, width, channel = image.shape\n","      if (height % 32) or (width % 32):\n","          image = cv2.resize(image, (416, 416)) ###\n","\n","      # initialize mask as None\n","      mask = None\n","\n","      if self.masks_path == None:\n","        if self.augmentation: ###plus besoin ?\n","          sample = self.augmentation(image=image)\n","          image = sample['image']\n","        if self.preprocessing:\n","            sample = self.preprocessing(image=image)\n","            image = sample['image']\n","        return self.images_path[i], image\n","\n","      else:\n","        mask = cv2.imread(self.masks_path[i], 0)\n","        if (height % 32) or (width % 32): ###\n","            mask = cv2.resize(mask, (416, 416))\n","            mask[mask<=120] = 0 #pixel value {0, 255}\n","            mask[mask>120] = 255\n","        masks = [(mask == v) for v in self.class_values]\n","        mask = np.stack(masks, axis=-1).astype('float')\n","        if self.augmentation:\n","            sample = self.augmentation(image=image, mask=mask)\n","            image, mask = sample['image'], sample['mask']\n","        if self.preprocessing:\n","              sample = self.preprocessing(image=image, mask=mask)\n","              image, mask = sample['image'], sample['mask']\n","        return image, mask\n","\n","  def __len__(self):\n","      return len(self.ids)"],"metadata":{"id":"SH8hhzxoqTgm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## II.b) Preprocessing"],"metadata":{"id":"vZ_X5BXZljDe"}},{"cell_type":"markdown","source":["### Data aug"],"metadata":{"id":"cIVkMNXdnf2N"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_TOoDIgZrYKY"},"outputs":[],"source":["# Create folders for data augmentation\n","if False:\n","  %mkdir data/data_train_augmented\n","  %mkdir data/data_train_augmented/images/\n","  %mkdir data/data_train_augmented/masks/\n","  %mkdir data/data_train_augmented/raw/\n","  %mkdir data/data_train_augmented/raw/images/\n","  %mkdir data/data_train_augmented/raw/masks/\n","  %mkdir data/data_validation\n","  %mkdir data/data_validation/images/\n","  %mkdir data/data_validation/masks/\n","  %mkdir data/data_validation/raw/\n","  %mkdir data/data_validation/raw/images/\n","  %mkdir data/data_validation/raw/masks/\n","\n","  # Load images and masks from dataset\n","  PATH_IMG_TRAIN = \"./data/training/images/\"\n","  PATH_MASK_TRAIN = \"./data/training/groundtruth/\"\n","  img_train, mask_train = load_img_training(PATH_IMG_TRAIN, PATH_MASK_TRAIN)\n","  key_list = list(img_train.keys())\n","  key_list.sort()\n","\n","  # Split the images for training/validation (+ store)\n","  training_ratio = 0.8\n","  seed = 1\n","  train_keys, val_keys = split_keys(np.array(key_list), training_ratio=training_ratio, seed=seed)\n","\n","  PATH_TR_IMG_AUG_RAW = \"./data/data_train_augmented/raw/images/\"\n","  PATH_TR_MASK_AUG_RAW = \"./data/data_train_augmented/raw/masks/\"\n","  PATH_VAL_IMG_RAW = \"./data/data_validation/raw/images/\"\n","  PATH_VAL_MASK_RAW = \"./data/data_validation/raw/masks/\"\n","\n","  store_images(img_train, train_keys, PATH_TR_IMG_AUG_RAW)\n","  store_images(mask_train, train_keys, PATH_TR_MASK_AUG_RAW)\n","  store_images(img_train, val_keys, PATH_VAL_IMG_RAW)\n","  store_images(mask_train, val_keys, PATH_VAL_MASK_RAW)\n","\n","  MASK_THRESHOLD = 120\n","  SIZE_X = 416 #divisible by 32\n","  SIZE_Y = 416 #divisible by 32\n","  PATH_TR_IMG_AUG = \"./data/data_train_augmented/images/\"\n","  PATH_TR_MASK_AUG = \"./data/data_train_augmented/masks/\"\n","  PATH_VAL_IMG = \"./data/data_validation/images/\"\n","  PATH_VAL_MASK = \"./data/data_validation/masks/\"\n","\n","  # Load validation images and resize\n","  img_val_raw, mask_val_raw = load_img_training(PATH_VAL_IMG_RAW, PATH_VAL_MASK_RAW)\n","  keys_val = list(img_val_raw.keys())\n","  resize_augment_store_dataset(img_val_raw, mask_val_raw, keys_val, SIZE_Y, SIZE_X, MASK_THRESHOLD, PATH_VAL_IMG, PATH_VAL_MASK, augment=False)\n","\n","  # Load training images, resize and augment using geometric transformation (+ store)\n","  img_tr_raw, mask_tr_raw = load_img_training(PATH_TR_IMG_AUG_RAW, PATH_TR_MASK_AUG_RAW)\n","  keys_tr = list(img_tr_raw.keys())\n","  resize_augment_store_dataset(img_tr_raw, mask_tr_raw, keys_tr, SIZE_Y, SIZE_X, MASK_THRESHOLD, PATH_TR_IMG_AUG, PATH_TR_MASK_AUG, augment=True)"]},{"cell_type":"markdown","source":["### Normalization"],"metadata":{"id":"mtj1DfhcniOd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MEoz5JEpaykY"},"outputs":[],"source":["def get_preprocessing(preprocessing_fn):\n","    \"\"\"Construct preprocessing transform\n","\n","    Args:\n","        preprocessing_fn (callbale): data normalization function\n","            (can be specific for each pretrained neural network)\n","    Return:\n","        transform: albumentations.Compose\n","\n","    \"\"\"\n","\n","    _transform = [\n","        albu.Lambda(image=preprocessing_fn),\n","        albu.Lambda(image=to_tensor, mask=to_tensor),\n","    ]\n","    return albu.Compose(_transform)\n","\n","def to_tensor(x, **kwargs):\n","    return x.transpose(2, 0, 1).astype('float32')"]},{"cell_type":"markdown","source":["## II.c) Dataset importation"],"metadata":{"id":"oX8tJT0jnqSO"}},{"cell_type":"code","source":["## Importing pre processing\n","preprocessing_fn = smp.encoders.get_preprocessing_fn(PARAMS[\"ENCODER\"], PARAMS[\"ENCODER_WEIGHTS\"])"],"metadata":{"id":"a_VBP_KYtM2j"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1CHZmMQaaykZ"},"outputs":[],"source":["PATH_TR_IMG_AUG_RAW = \"./data/data_train_augmented/raw/images/\"\n","PATH_TR_MASK_AUG_RAW = \"./data/data_train_augmented/raw/masks/\"\n","PATH_VAL_IMG_RAW = \"./data/data_validation/raw/images/\"\n","PATH_VAL_MASK_RAW = \"./data/data_validation/raw/masks/\"\n","PATH_TR_IMG_AUG = \"./data/data_train_augmented/images/\"\n","PATH_TR_MASK_AUG = \"./data/data_train_augmented/masks/\"\n","PATH_VAL_IMG = \"./data/data_validation/images/\"\n","PATH_VAL_MASK = \"./data/data_validation/masks/\"\n","\n","\n","#change paths for the training and validation datasets depending on wether we want data augmentation or not\n","if PARAMS[\"DATA_AUGMENTATION\"]:\n","  training_path_img = PATH_TR_IMG_AUG\n","  training_path_mask = PATH_TR_MASK_AUG\n","  validation_path_img = PATH_VAL_IMG\n","  validation_path_mask = PATH_VAL_MASK\n","else:\n","  training_path_img = PATH_TR_IMG_AUG_RAW\n","  training_path_mask = PATH_TR_MASK_AUG_RAW\n","  validation_path_img = PATH_VAL_IMG_RAW\n","  validation_path_mask = PATH_VAL_MASK_RAW\n","\n","#create training and validation datasets\n","train_dataset = Dataset(\n","    training_path_img,\n","    training_path_mask,\n","    preprocessing=get_preprocessing(preprocessing_fn),\n","    classes=[\"road\"])\n","\n","\n","valid_dataset = Dataset(\n","    validation_path_img,\n","    validation_path_mask,\n","    preprocessing=get_preprocessing(preprocessing_fn),\n","    classes=[\"road\"],\n",")\n","\n","#create the loaders for both datasets\n","train_loader = DataLoader(train_dataset, batch_size=30, shuffle=False)\n","valid_loader = DataLoader(valid_dataset, batch_size=20, shuffle=False)\n","\n"]},{"cell_type":"markdown","source":["## II.d) Loss and evaluation metrics"],"metadata":{"id":"3RT_9a1gnzOO"}},{"cell_type":"markdown","source":["### Patch based Loss and evaluation metrics\n"],"metadata":{"id":"xNLXJTNEn91d"}},{"cell_type":"code","source":["class F1score_patch(smp.utils.base.Metric):\n","    def __init__(self, threshold=0.5, activation=None, patch_thr=0.25, patch_size=16, **kwargs):\n","        super().__init__(**kwargs)\n","        self.threshold = threshold\n","        self.activation = smp.base.modules.Activation(activation)\n","        self.patch_thr = patch_thr\n","        self.patch_size = patch_size\n","\n","    def forward(self, y_pr, y_gt):  # pr => predicted, gt => groundtruth\n","        y_pr = self.activation(y_pr)\n","        y_pr = (y_pr > self.threshold).float()  # value 0.0 or 1.0\n","\n","        y_gt = self.activation(y_gt)\n","        y_gt = (y_gt > self.threshold).float()  # value 0.0 or 1.0\n","\n","        batch_size, nb_channels, height, width = y_pr.size()\n","\n","        y_pr_patch_tensor = torch.zeros(batch_size, nb_channels, height//self.patch_size, width//self.patch_size)  # N, C, H, W\n","        y_gt_patch_tensor = torch.zeros(batch_size, nb_channels, height//self.patch_size, width//self.patch_size)\n","\n","        for y in range(0, height, self.patch_size):\n","            for x in range(0, width, self.patch_size):\n","                # Extract the patches of the batch\n","                patches_pr = y_pr[..., y:y + self.patch_size, x:x + self.patch_size]\n","                patches_gt = y_gt[..., y:y + self.patch_size, x:x + self.patch_size]\n","\n","                # Iterate through each patch of the prediction\n","                for i, patch_pr in enumerate(patches_pr):\n","                    # Calculate the average of the patch\n","                    patch_avg_pr = torch.mean(patch_pr)\n","                    # Patch threshold\n","                    if patch_avg_pr > self.patch_thr:\n","                        y_pr_patch_tensor[i][0][y//self.patch_size][x//self.patch_size] = 1\n","                    else:\n","                        y_pr_patch_tensor[i][0][y//self.patch_size][x//self.patch_size] = 0\n","\n","                # Iterate through each patch of the groundtruth mask\n","                for i, patch_gt in enumerate(patches_gt):\n","                    # Calculate the average of the patch\n","                    patch_avg_gt = torch.mean(patch_gt)\n","                    # Patch threshold\n","                    if patch_avg_gt > self.patch_thr:\n","                        y_gt_patch_tensor[i][0][y//self.patch_size][x//self.patch_size] = 1\n","                    else:\n","                        y_gt_patch_tensor[i][0][y//self.patch_size][x//self.patch_size] = 0\n","\n","        tp, fp, fn, tn = smp.metrics.get_stats(y_pr_patch_tensor.to(torch.int), y_gt_patch_tensor.to(torch.int), mode='binary', threshold=self.threshold)\n","        f1_score_patch = smp.metrics.f1_score(tp=tp, fp=fp, fn=fn, tn=tn, reduction=\"micro-imagewise\") #reduction=\"micro\" or \"micro-imagewise\"\n","\n","        return f1_score_patch"],"metadata":{"id":"Eh0kbVVA9Wid"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the image crop function\n","def img_crop(im, h, w):\n","    list_patches = []\n","    imgwidth = im.shape[2]\n","    imgheight = im.shape[3]\n","    for i in range(0, imgheight, h):\n","        for j in range(0, imgwidth, w):\n","            im_patch = im[:, :, j:j+w, i:i+h]\n","            list_patches.append((im_patch, i, j))\n","    return list_patches\n","\n","# Define the value to class conversion function\n","def value_to_class(v, threshold):\n","    v_mean = v.mean(dim=(2, 3))\n","    return (v_mean > threshold).float()\n","\n","def normalize_tensor(tensor):\n","    N, _, _, _ = tensor.shape\n","    normalized_tensor = torch.zeros_like(tensor)\n","    for i in range(N):\n","        min_val = tensor[i].min()\n","        max_val = tensor[i].max()\n","        normalized_tensor[i] = (tensor[i] - min_val) / (max_val - min_val)\n","    return normalized_tensor\n","\n","# Assuming predictions_tensor and ground_truths_tensor are the input 4D tensors\n","def patch_extraction(predictions_tensor, ground_truths_tensor, threshold_pred=0.25, threshold_gt=0.25, print = False, apply_sigmoid = True, patch_size = 16):\n","    N, C, H, W = predictions_tensor.shape\n","    num_patches_height = H // patch_size\n","    num_patches_width = W // patch_size\n","\n","    # Initialize tensors to hold the mean values\n","    predictions_means = torch.zeros((N, C, num_patches_height, num_patches_width))\n","    ground_truths_means = torch.zeros((N, C, num_patches_height, num_patches_width))\n","\n","    # Pass through a sigmoid to normalize data between 0 and 1\n","    predictions_tensor_pure = predictions_tensor.cpu()\n","    if apply_sigmoid:\n","        predictions_tensor = torch.sigmoid(predictions_tensor).cpu()\n","        predictions_tensor_classes = (predictions_tensor > 0.5).float()\n","\n","    # Crop the images into patches and compute the mean values\n","    # predictions_patches = img_crop(predictions_tensor, patch_size, patch_size)\n","    predictions_patches = img_crop(predictions_tensor_classes, patch_size, patch_size)\n","    ground_truths_patches = img_crop(ground_truths_tensor, patch_size, patch_size)\n","\n","    # Assign the mean values to the correct location in the tensors\n","    for patch, i, j in predictions_patches:\n","        patch_idx_height = j // patch_size\n","        patch_idx_width = i // patch_size\n","        predictions_means[:, :, patch_idx_height, patch_idx_width] = patch.mean(dim=(2, 3))\n","\n","    for patch, i, j in ground_truths_patches:\n","        patch_idx_height = j // patch_size\n","        patch_idx_width = i // patch_size\n","        ground_truths_means[:, :, patch_idx_height, patch_idx_width] = patch.mean(dim=(2, 3))\n","\n","    # Apply threshold to determine class membership\n","    predictions_classes = (predictions_means > threshold_pred).float()\n","    ground_truths_classes = (ground_truths_means > threshold_gt).float()\n","\n","    # Plot the mean images\n","    if print:\n","      fig, ax = plt.subplots(2, 2, figsize=(10, 5))\n","      ax[0, 0].imshow(predictions_tensor_pure[0].squeeze().detach().numpy(), cmap='gray')  # Detach and convert to numpy\n","      ax[0, 0].set_title('predictions_tensor_pure')\n","      ax[0, 1].imshow(predictions_means[0].squeeze().detach().numpy(), cmap='gray')  # Detach and convert to numpy\n","      ax[0, 1].set_title('predictions_means sigmoid')\n","      ax[1, 0].imshow(predictions_classes[0].squeeze().detach().numpy(), cmap='gray')  # Detach and convert to numpy\n","      ax[1, 0].set_title('predictions_classes')\n","      ax[1, 1].imshow(ground_truths_classes[0].squeeze().detach().numpy(), cmap='gray')  # Detach and convert to numpy\n","      ax[1, 1].set_title('ground_truths_classes')\n","      plt.show()\n","\n","      # Plot histograms\n","      plt.figure(figsize=(10, 5))\n","      plt.subplot(1, 2, 1)\n","      plt.hist(predictions_means[0].cpu().squeeze().detach().numpy().flatten(), bins=50, color='blue', alpha=0.7, label='Prediction Means')\n","      plt.title('predictions_means')\n","      plt.xlabel('Pixel Values')\n","      plt.ylabel('Frequency')\n","      plt.legend()\n","\n","      plt.subplot(1, 2, 2)\n","      plt.hist(ground_truths_means[0].cpu().squeeze().detach().numpy().flatten(), bins=50, color='green', alpha=0.7, label='Ground Truth Means')\n","      plt.title('ground_truths_means')\n","      plt.xlabel('Pixel Values')\n","      plt.ylabel('Frequency')\n","      plt.legend()\n","      plt.tight_layout()\n","      plt.show()\n","\n","      # Plot histograms\n","      plt.figure(figsize=(10, 5))\n","      plt.subplot(1, 2, 1)\n","      plt.hist(predictions_tensor_pure[0].cpu().squeeze().detach().numpy().flatten(), bins=50, color='blue', alpha=0.7, label='Prediction Means')\n","      plt.title('predictions_tensor_pure')\n","      plt.xlabel('Pixel Values')\n","      plt.ylabel('Frequency')\n","      plt.legend()\n","\n","      plt.subplot(1, 2, 2)\n","      plt.hist(predictions_tensor[0].cpu().squeeze().detach().numpy().flatten(), bins=50, color='green', alpha=0.7, label='Ground Truth Means')\n","      plt.title('predictions_tensor sigmoid')\n","      plt.xlabel('Pixel Values')\n","      plt.ylabel('Frequency')\n","      plt.legend()\n","\n","      plt.tight_layout()\n","      plt.show()\n","\n","    return predictions_classes, ground_truths_classes"],"metadata":{"id":"y8bauN8IoDie"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Composite metrics\n","\n","class Custom_Fscore(base.Metric):\n","    def __init__(self, beta=1, eps=1e-7, threshold=0.5, activation=None, ignore_channels=None, w_pix = 1, w_patch = 1, **kwargs):\n","        super(Custom_Fscore,self).__init__(**kwargs)\n","        self.eps = eps\n","        self.beta = beta\n","        self.threshold = threshold\n","        self.activation = Activation(activation)\n","        self.ignore_channels = ignore_channels\n","        self.w_pix = w_pix\n","        self.w_patch = w_patch\n","        print(\"Fscore w_pix: \", w_pix)\n","        print(\"Fscore w_patch: \", w_patch)\n","\n","    def forward(self, pr, gt):\n","        pr = self.activation(pr)\n","        pr_pix = pr\n","        gt_pix = gt\n","        pr_patch, gt_patch = patch_extraction(pr, gt)\n","\n","        f_score_pix = smp_utils.metrics.Fscore(\n","            beta=self.beta,\n","            eps=self.eps,\n","            threshold=self.threshold,\n","            ignore_channels=self.ignore_channels,\n","        ).forward(pr_pix, gt_pix)\n","\n","        f_score_patch = smp_utils.metrics.Fscore(\n","            beta=self.beta,\n","            eps=self.eps,\n","            threshold=self.threshold,\n","            ignore_channels=self.ignore_channels,\n","        ).forward(pr_patch, gt_patch)\n","\n","        return (self.w_pix * f_score_pix + self.w_patch * f_score_patch) / (self.w_pix + self.w_patch)"],"metadata":{"id":"82YDNrXwoDem"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Final model choice"],"metadata":{"id":"zR84iWPhoBTd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"764Vgxn2ayka"},"outputs":[],"source":["# Dice/F1 score - https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n","# IoU/Jaccard score - https://en.wikipedia.org/wiki/Jaccard_index\n","\n","# loss = smp_utils.losses.DiceLoss()\n","# metrics = [\n","#     smp_utils.metrics.IoU(threshold=0.5),\n","#     smp_utils.metrics.Fscore(),\n","# ]\n","#myinstance.__class__.__name__\n","\n","# loss_fn = CustomDiceLoss(w_pix = w_pix, w_patch = w_patch)\n","# loss_fn.__name__ = 'Custom_dice_loss'\n","\n","# metrics = [\n","#       Custom_Fscore(w_pix = w_pix, w_patch = w_patch), ]\n","\n","if loss_type == \"dice\":\n","  loss_fn = smp.losses.dice.DiceLoss(mode ='binary')\n","  loss_fn.__name__ = 'Dice_loss'\n","  loss_name = 'Dice_loss'\n","elif loss_type == \"tversky\":\n","  loss_fn = smp.losses.tversky.TverskyLoss(mode ='binary')\n","  loss_fn.__name__ = 'Tversky_Loss'\n","  loss_name = 'Tversky_Loss'\n","elif loss_type == \"custom\":\n","  print(\"Not implemented yet\")\n","\n","if metric_type == \"custom\":\n","  metrics = [\n","        Custom_Fscore(w_pix = w_pix, w_patch = w_patch), ]\n","  metric_name = \"custom__fscore\"\n","elif metric_type == \"fscore\":\n","  metrics = [\n","        F1score_patch(activation='sigmoid')] #smp_utils.metrics.Fscore()\n","  metric_name = \"f1score_patch\""]},{"cell_type":"markdown","source":["# III) Training"],"metadata":{"id":"l-P1uVn8mAIN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cd2imBFfaykX"},"outputs":[],"source":["models = [[smp.create_model(model_name, encoder_name=PARAMS[\"ENCODER\"], encoder_weights = PARAMS[\"ENCODER_WEIGHTS\"], in_channels=3, classes=1),model_name] for model_name in PARAMS[\"MODELS\"]]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":626},"executionInfo":{"elapsed":148219,"status":"ok","timestamp":1701986137988,"user":{"displayName":"Edwin Bertschy","userId":"01520589558980144494"},"user_tz":-60},"id":"h2DZo52faykc","outputId":"0340896b-0e0f-4af9-9676-b929dae38b2a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Unet\n","\n","Epoch: 0\n","train: 100%|██████████| 32/32 [02:23<00:00,  4.49s/it, Dice_loss - 0.5859, f1score_patch - 0.5114]\n","valid: 100%|██████████| 1/1 [00:02<00:00,  2.79s/it, Dice_loss - 0.507, f1score_patch - 0.6293]\n","{'Dice_loss': 0.5069621801376343, 'f1score_patch': 0.629327118396759}\n","Model saved!\n","test\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x500 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# train model for NB_EPOCHS\n","for model,model_name in models:\n","  print(model_name)\n","  optimizer = torch.optim.Adam([\n","    dict(params=model.parameters(), lr=0.0001),\n","])\n","\n","  train_epoch = smp.utils.train.TrainEpoch(\n","      model,\n","      loss=loss_fn,\n","      metrics=metrics,\n","      optimizer=optimizer,\n","      device=\"cuda\",\n","      verbose=True,\n","  )\n","\n","  valid_epoch = smp.utils.train.ValidEpoch(\n","      model,\n","      loss=loss_fn,\n","      metrics=[F1score_patch(activation='sigmoid')], #metrics\n","      device=\"cuda\",\n","      verbose=True,\n","  )\n","\n","\n","  max_score = 0\n","  train_loss_array = []\n","  validation_loss_array = []\n","  validation_fscore_array = []\n","\n","\n","\n","  for i in range(0, PARAMS[\"NB_EPOCHS\"]):\n","\n","      print('\\nEpoch: {}'.format(i))\n","      train_logs = train_epoch.run(train_loader)\n","      valid_logs = valid_epoch.run(valid_loader)\n","\n","      train_loss_array.append(train_logs[loss_name])\n","      validation_loss_array.append(valid_logs[loss_name])\n","      validation_fscore_array.append(valid_logs[metric_name])\n","      print(valid_logs)\n","      # do something (save model, change lr, etc.)\n","      if max_score < valid_logs[metric_name]:\n","          max_score = valid_logs[metric_name]\n","          torch.save(model, model_weights_folder + 'best_model_{}.pth'.format(model_name))\n","          print('Model saved!')\n","\n","      # if i == 25:\n","          # optimizer.param_groups[0]['lr'] = 1e-5\n","          # print('Decrease decoder learning rate to 1e-5!')\n","  epochs = range(0,len(train_loss_array))\n","\n","  save_results(PARAMS,train_loss_array,validation_loss_array,validation_fscore_array)\n","\n","\n","  plt.figure(figsize=(10, 5))\n","  plt.plot(epochs, train_loss_array,\"o\", label='Training Loss')\n","  plt.plot(epochs, validation_loss_array,  label='Validation Loss')\n","  plt.plot(epochs, validation_fscore_array, \"o\" ,  label='Validation fscore')\n","  plt.title('Training and Validation Loss for {}'.format(model_name))\n","  plt.xlabel('Epochs')\n","  plt.ylabel('Loss')\n","  plt.legend()\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gb7gjUmf17ON"},"source":["# IV) Submission\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HBx4LAICmMHa"},"outputs":[],"source":["MODEL_NAME = \"Unet\"\n","# Modify according to model saved that you want to import\n","MODEL_PATH = model_weights_folder + f'best_model_{MODEL_NAME}.pth'\n","DEVICE ='cuda'\n","test_model = torch.load(MODEL_PATH)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q56JyGLW17OS"},"outputs":[],"source":["test_dataset = Dataset(\n","    images_dir=\"./data/test_set_images/\",\n","    preprocessing= get_preprocessing(preprocessing_fn),\n","    classes=['road'])\n","\n","test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)"]},{"cell_type":"markdown","metadata":{"id":"YNaRNAT417OR"},"source":["### Visualization of results"]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import cv2\n","import numpy as np\n","import torch\n","import os\n","\n","# Ensure the model is on the correct device and in evaluation mode\n","test_model = test_model.to(DEVICE)\n","test_model.eval()\n","activate_threshold = True\n","img_nbr = 10\n","i = 0\n","# Iterate over all batches in the test_loader\n","for batch in test_loader:\n","    i += 1\n","    with torch.no_grad():\n","        # Move input to the device where the model is\n","        input_tensors = batch[1].to(DEVICE)\n","        logits = test_model(input_tensors)\n","        pr_gts = logits.sigmoid()\n","\n","        # Since there are no ground truth masks, we only visualize the images and predictions\n","        for img_fp, pr_gt in zip(batch[0], pr_gts):  # batch[0] should contain the file paths\n","            print(img_fp)\n","            img = cv2.imread(img_fp)\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","            img = cv2.resize(img, (416, 416))\n","\n","            # Get the prediction as a numpy array\n","            pr_gt_cpu = pr_gt.cpu().numpy().squeeze()\n","            if activate_threshold:\n","              pr_gt_cpu[pr_gt_cpu >= foreground_threshold] = 1\n","              pr_gt_cpu[pr_gt_cpu < foreground_threshold] = 0\n","            plt.figure(figsize=(10, 5))\n","\n","            plt.subplot(1, 2, 1)\n","            plt.imshow(img)  # No need to transpose axes since img is read with cv2 and already in HWC format\n","            plt.title(\"Image\")\n","            plt.axis(\"off\")\n","\n","            plt.subplot(1, 2, 2)\n","            plt.imshow(pr_gt_cpu, cmap='gray')  # Show the prediction\n","            plt.title(\"Prediction\")\n","            plt.axis(\"off\")\n","\n","            plt.show()\n","    if i == img_nbr:\n","      break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1HUDr6QCwCqe0CpXOE-cTjG5sl_TH8_1q"},"id":"BM8NQpnDtEYt","executionInfo":{"status":"ok","timestamp":1701986144121,"user_tz":-60,"elapsed":5775,"user":{"displayName":"Edwin Bertschy","userId":"01520589558980144494"}},"outputId":"8f8260aa-3972-4b54-bd39-abf2507c3f7d"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["## Submission creation"],"metadata":{"id":"3REiwYfLnMEF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cBa0US5H17OW"},"outputs":[],"source":["DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","test_model = test_model.to(DEVICE)\n","\n","# Parameters for saving images\n","output_folder = submission_folder + 'eval_imgs/'\n","os.makedirs(output_folder, exist_ok=True)\n","\n","# Iterate over the DataLoader\n","for i, (path, image) in enumerate(test_loader):\n","    # print(\"Processing image:\", i)\n","    path = path[0]\n","    with torch.no_grad():\n","        test_model.eval()\n","        # Move input to the device\n","        input_tensor = image.to(DEVICE)\n","        logits = test_model(input_tensor)\n","        # Apply sigmoid to get probabilities\n","        probabilities = torch.sigmoid(logits)\n","        # Squeeze to remove unnecessary dimensions\n","        probabilities = probabilities.squeeze(0).squeeze(0)\n","        # Apply threshold to the probabilities to binarize\n","        prediction_binarized = (probabilities > foreground_threshold).float() ### <\n","\n","        # Convert the binarized prediction to a PIL image\n","        prediction_pil = to_pil_image(prediction_binarized)\n","\n","        # Resize the PIL image to 608x608\n","        # prediction_pil_resized = prediction_pil.resize((608, 608), Image.NEAREST)\n","\n","        # Save the image\n","        image_num = path.split('/')[-1].split('_')[-1].split('.')[0]\n","        image_num = int(image_num)\n","        filename = \"test_eval_\" + '%.3d' % image_num + '.png'\n","        prediction_pil.save(os.path.join(output_folder, filename))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OgXcwZ4K17OX"},"outputs":[],"source":["submission_filename = submission_folder + 'test_submission.csv'\n","image_filenames = []\n","for i in range(1, 51):\n","    # image_filename = 'training/groundtruth/satImage_' + '%.3d' % i + '.png'\n","    image_filename = submission_folder + 'eval_imgs/test_eval_' + '%.3d' % i + '.png'\n","    image_filenames.append(image_filename)\n","masks_to_submission(submission_filename, *image_filenames)\n"]},{"cell_type":"markdown","source":["## Visualisation predictions"],"metadata":{"id":"d_cqzcgW-6uS"}},{"cell_type":"code","source":["if False:\n","    def list_files_in_directory(directory):\n","        file_list = []\n","        for root, dirs, files in os.walk(directory):\n","            for file in files:\n","                file_list.append(os.path.join(root, file))\n","        return file_list\n","\n","    pred_path = submission_folder + \"/eval_imgs/\"\n","    pred_files = list_files_in_directory(pred_path)\n","\n","    img_path = \"./data/test_set_images/\"\n","    img_files = list_files_in_directory(img_path)\n","    img_files = sorted(img_files, key=lambda x: int(os.path.basename(x)[5:-4]))\n","    img_mask_list = []\n","    for i, pred_file in enumerate(pred_files):\n","        img = cv2.imread(img_files[i], cv2.IMREAD_COLOR)\n","        pred_mask = cv2.imread(pred_file, cv2.IMREAD_GRAYSCALE)\n","\n","        # Create a semi-transparent green mask\n","        alpha = 0.7\n","        red_mask = np.zeros_like(img)\n","        for (i,j), value in np.ndenumerate(pred_mask):\n","            if value == 255:\n","                red_mask[i,j] = [0, 255, 0]\n","\n","        # Combine image and the green mask\n","        result = cv2.addWeighted(img, 1, red_mask, alpha, 0)\n","        img_mask_list.append(result)\n","    # Set the number of images per row\n","    images_per_row = 2\n","\n","    # Calculate the number of rows needed\n","    num_rows = (len(img_mask_list) + images_per_row - 1) // images_per_row\n","\n","    # Create a single row of subplots\n","    fig, axs = plt.subplots(1, images_per_row, figsize=(10, 5))\n","\n","    # Loop through files and plot images\n","    for i, img in enumerate(img_mask_list):\n","        axs[i % images_per_row].imshow(img)\n","        axs[i % images_per_row].axis('off')\n","\n","        # If last image in the row, create a new row of subplots\n","        if (i + 1) % images_per_row == 0:\n","            plt.show()\n","            if i + 1 < len(img_mask_list):\n","                fig, axs = plt.subplots(1, images_per_row, figsize=(10, 5))\n","\n","    plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1b79r4QhXzqc3VPqdf1Be8xAVVTXLsFMm"},"id":"e52giDyD-47e","executionInfo":{"status":"ok","timestamp":1701986212169,"user_tz":-60,"elapsed":63836,"user":{"displayName":"Edwin Bertschy","userId":"01520589558980144494"}},"outputId":"83c04ecf-4050-4967-f955-c9a7603b482e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"collapsed_sections":["YNaRNAT417OR"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}